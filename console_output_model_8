I1206 10:40:45.160213  2840 caffe.cpp:217] Using GPUs 0
I1206 10:40:45.198922  2840 caffe.cpp:222] GPU 0: NVIDIA Tegra X1
I1206 10:40:46.082628  2840 solver.cpp:60] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
I1206 10:40:46.083688  2840 solver.cpp:103] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I1206 10:40:46.084508  2840 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1206 10:40:46.084628  2840 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1206 10:40:46.084710  2840 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1206 10:40:46.087949  2840 layer_factory.hpp:77] Creating layer mnist
I1206 10:40:46.089442  2840 net.cpp:100] Creating Layer mnist
I1206 10:40:46.089536  2840 net.cpp:408] mnist -> data
I1206 10:40:46.089673  2840 net.cpp:408] mnist -> label
I1206 10:40:46.092778  2848 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I1206 10:40:46.250336  2840 data_layer.cpp:41] output data size: 64,1,28,28
I1206 10:40:46.254412  2840 net.cpp:150] Setting up mnist
I1206 10:40:46.254483  2840 net.cpp:157] Top shape: 64 1 28 28 (50176)
I1206 10:40:46.254552  2840 net.cpp:157] Top shape: 64 (64)
I1206 10:40:46.254595  2840 net.cpp:165] Memory required for data: 200960
I1206 10:40:46.254644  2840 layer_factory.hpp:77] Creating layer conv1
I1206 10:40:46.254736  2840 net.cpp:100] Creating Layer conv1
I1206 10:40:46.254783  2840 net.cpp:434] conv1 <- data
I1206 10:40:46.254842  2840 net.cpp:408] conv1 -> conv1
I1206 10:40:47.635625  2840 net.cpp:150] Setting up conv1
I1206 10:40:47.635699  2840 net.cpp:157] Top shape: 64 20 24 24 (737280)
I1206 10:40:47.635799  2840 net.cpp:165] Memory required for data: 3150080
I1206 10:40:47.635880  2840 layer_factory.hpp:77] Creating layer pool1
I1206 10:40:47.636003  2840 net.cpp:100] Creating Layer pool1
I1206 10:40:47.636046  2840 net.cpp:434] pool1 <- conv1
I1206 10:40:47.636090  2840 net.cpp:408] pool1 -> pool1
I1206 10:40:47.636276  2840 net.cpp:150] Setting up pool1
I1206 10:40:47.636313  2840 net.cpp:157] Top shape: 64 20 12 12 (184320)
I1206 10:40:47.636353  2840 net.cpp:165] Memory required for data: 3887360
I1206 10:40:47.636389  2840 layer_factory.hpp:77] Creating layer conv2
I1206 10:40:47.636435  2840 net.cpp:100] Creating Layer conv2
I1206 10:40:47.636471  2840 net.cpp:434] conv2 <- pool1
I1206 10:40:47.636515  2840 net.cpp:408] conv2 -> conv2
I1206 10:40:47.642520  2840 net.cpp:150] Setting up conv2
I1206 10:40:47.642576  2840 net.cpp:157] Top shape: 64 50 8 8 (204800)
I1206 10:40:47.642619  2840 net.cpp:165] Memory required for data: 4706560
I1206 10:40:47.642674  2840 layer_factory.hpp:77] Creating layer pool2
I1206 10:40:47.642721  2840 net.cpp:100] Creating Layer pool2
I1206 10:40:47.642752  2840 net.cpp:434] pool2 <- conv2
I1206 10:40:47.642793  2840 net.cpp:408] pool2 -> pool2
I1206 10:40:47.642953  2840 net.cpp:150] Setting up pool2
I1206 10:40:47.642990  2840 net.cpp:157] Top shape: 64 50 4 4 (51200)
I1206 10:40:47.643029  2840 net.cpp:165] Memory required for data: 4911360
I1206 10:40:47.643064  2840 layer_factory.hpp:77] Creating layer ip1
I1206 10:40:47.643112  2840 net.cpp:100] Creating Layer ip1
I1206 10:40:47.643147  2840 net.cpp:434] ip1 <- pool2
I1206 10:40:47.643183  2840 net.cpp:408] ip1 -> ip1
I1206 10:40:47.652856  2840 net.cpp:150] Setting up ip1
I1206 10:40:47.652927  2840 net.cpp:157] Top shape: 64 500 (32000)
I1206 10:40:47.652973  2840 net.cpp:165] Memory required for data: 5039360
I1206 10:40:47.653030  2840 layer_factory.hpp:77] Creating layer relu1
I1206 10:40:47.653087  2840 net.cpp:100] Creating Layer relu1
I1206 10:40:47.653125  2840 net.cpp:434] relu1 <- ip1
I1206 10:40:47.653174  2840 net.cpp:395] relu1 -> ip1 (in-place)
I1206 10:40:47.655189  2840 net.cpp:150] Setting up relu1
I1206 10:40:47.655252  2840 net.cpp:157] Top shape: 64 500 (32000)
I1206 10:40:47.655294  2840 net.cpp:165] Memory required for data: 5167360
I1206 10:40:47.655388  2840 layer_factory.hpp:77] Creating layer ip2
I1206 10:40:47.655433  2840 net.cpp:100] Creating Layer ip2
I1206 10:40:47.655463  2840 net.cpp:434] ip2 <- ip1
I1206 10:40:47.655499  2840 net.cpp:408] ip2 -> ip2
I1206 10:40:47.656497  2840 net.cpp:150] Setting up ip2
I1206 10:40:47.656551  2840 net.cpp:157] Top shape: 64 10 (640)
I1206 10:40:47.656594  2840 net.cpp:165] Memory required for data: 5169920
I1206 10:40:47.656635  2840 layer_factory.hpp:77] Creating layer loss
I1206 10:40:47.656687  2840 net.cpp:100] Creating Layer loss
I1206 10:40:47.656728  2840 net.cpp:434] loss <- ip2
I1206 10:40:47.656765  2840 net.cpp:434] loss <- label
I1206 10:40:47.656805  2840 net.cpp:408] loss -> loss
I1206 10:40:47.656873  2840 layer_factory.hpp:77] Creating layer loss
I1206 10:40:47.658895  2840 net.cpp:150] Setting up loss
I1206 10:40:47.658949  2840 net.cpp:157] Top shape: (1)
I1206 10:40:47.658987  2840 net.cpp:160]     with loss weight 1
I1206 10:40:47.659051  2840 net.cpp:165] Memory required for data: 5169924
I1206 10:40:47.659096  2840 net.cpp:226] loss needs backward computation.
I1206 10:40:47.659138  2840 net.cpp:226] ip2 needs backward computation.
I1206 10:40:47.659171  2840 net.cpp:226] relu1 needs backward computation.
I1206 10:40:47.659245  2840 net.cpp:226] ip1 needs backward computation.
I1206 10:40:47.659278  2840 net.cpp:226] pool2 needs backward computation.
I1206 10:40:47.659307  2840 net.cpp:226] conv2 needs backward computation.
I1206 10:40:47.659337  2840 net.cpp:226] pool1 needs backward computation.
I1206 10:40:47.659368  2840 net.cpp:226] conv1 needs backward computation.
I1206 10:40:47.659396  2840 net.cpp:228] mnist does not need backward computation.
I1206 10:40:47.659435  2840 net.cpp:270] This network produces output loss
I1206 10:40:47.659485  2840 net.cpp:283] Network initialization done.
I1206 10:40:47.659935  2840 solver.cpp:193] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I1206 10:40:47.660085  2840 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1206 10:40:47.660138  2840 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1206 10:40:47.662611  2840 layer_factory.hpp:77] Creating layer mnist
I1206 10:40:47.662894  2840 net.cpp:100] Creating Layer mnist
I1206 10:40:47.662952  2840 net.cpp:408] mnist -> data
I1206 10:40:47.663002  2840 net.cpp:408] mnist -> label
I1206 10:40:47.665906  2850 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I1206 10:40:47.671838  2840 data_layer.cpp:41] output data size: 100,1,28,28
I1206 10:40:47.685292  2840 net.cpp:150] Setting up mnist
I1206 10:40:47.685384  2840 net.cpp:157] Top shape: 100 1 28 28 (78400)
I1206 10:40:47.685446  2840 net.cpp:157] Top shape: 100 (100)
I1206 10:40:47.685508  2840 net.cpp:165] Memory required for data: 314000
I1206 10:40:47.685557  2840 layer_factory.hpp:77] Creating layer label_mnist_1_split
I1206 10:40:47.685627  2840 net.cpp:100] Creating Layer label_mnist_1_split
I1206 10:40:47.685678  2840 net.cpp:434] label_mnist_1_split <- label
I1206 10:40:47.685740  2840 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I1206 10:40:47.685820  2840 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I1206 10:40:47.686063  2840 net.cpp:150] Setting up label_mnist_1_split
I1206 10:40:47.686118  2840 net.cpp:157] Top shape: 100 (100)
I1206 10:40:47.686167  2840 net.cpp:157] Top shape: 100 (100)
I1206 10:40:47.686213  2840 net.cpp:165] Memory required for data: 314800
I1206 10:40:47.686259  2840 layer_factory.hpp:77] Creating layer conv1
I1206 10:40:47.686328  2840 net.cpp:100] Creating Layer conv1
I1206 10:40:47.686413  2840 net.cpp:434] conv1 <- data
I1206 10:40:47.686466  2840 net.cpp:408] conv1 -> conv1
I1206 10:40:47.697896  2840 net.cpp:150] Setting up conv1
I1206 10:40:47.697989  2840 net.cpp:157] Top shape: 100 20 24 24 (1152000)
I1206 10:40:47.698163  2840 net.cpp:165] Memory required for data: 4922800
I1206 10:40:47.698279  2840 layer_factory.hpp:77] Creating layer pool1
I1206 10:40:47.698346  2840 net.cpp:100] Creating Layer pool1
I1206 10:40:47.698402  2840 net.cpp:434] pool1 <- conv1
I1206 10:40:47.698473  2840 net.cpp:408] pool1 -> pool1
I1206 10:40:47.698788  2840 net.cpp:150] Setting up pool1
I1206 10:40:47.698844  2840 net.cpp:157] Top shape: 100 20 12 12 (288000)
I1206 10:40:47.698909  2840 net.cpp:165] Memory required for data: 6074800
I1206 10:40:47.698959  2840 layer_factory.hpp:77] Creating layer conv2
I1206 10:40:47.699044  2840 net.cpp:100] Creating Layer conv2
I1206 10:40:47.699103  2840 net.cpp:434] conv2 <- pool1
I1206 10:40:47.699195  2840 net.cpp:408] conv2 -> conv2
I1206 10:40:47.712981  2840 net.cpp:150] Setting up conv2
I1206 10:40:47.713048  2840 net.cpp:157] Top shape: 100 50 8 8 (320000)
I1206 10:40:47.713096  2840 net.cpp:165] Memory required for data: 7354800
I1206 10:40:47.713152  2840 layer_factory.hpp:77] Creating layer pool2
I1206 10:40:47.713207  2840 net.cpp:100] Creating Layer pool2
I1206 10:40:47.713243  2840 net.cpp:434] pool2 <- conv2
I1206 10:40:47.713759  2840 net.cpp:408] pool2 -> pool2
I1206 10:40:47.713997  2840 net.cpp:150] Setting up pool2
I1206 10:40:47.714035  2840 net.cpp:157] Top shape: 100 50 4 4 (80000)
I1206 10:40:47.714071  2840 net.cpp:165] Memory required for data: 7674800
I1206 10:40:47.714102  2840 layer_factory.hpp:77] Creating layer ip1
I1206 10:40:47.714150  2840 net.cpp:100] Creating Layer ip1
I1206 10:40:47.714182  2840 net.cpp:434] ip1 <- pool2
I1206 10:40:47.714220  2840 net.cpp:408] ip1 -> ip1
I1206 10:40:47.725843  2840 net.cpp:150] Setting up ip1
I1206 10:40:47.725914  2840 net.cpp:157] Top shape: 100 500 (50000)
I1206 10:40:47.725953  2840 net.cpp:165] Memory required for data: 7874800
I1206 10:40:47.726002  2840 layer_factory.hpp:77] Creating layer relu1
I1206 10:40:47.726049  2840 net.cpp:100] Creating Layer relu1
I1206 10:40:47.726083  2840 net.cpp:434] relu1 <- ip1
I1206 10:40:47.726116  2840 net.cpp:395] relu1 -> ip1 (in-place)
I1206 10:40:47.728677  2840 net.cpp:150] Setting up relu1
I1206 10:40:47.728781  2840 net.cpp:157] Top shape: 100 500 (50000)
I1206 10:40:47.728824  2840 net.cpp:165] Memory required for data: 8074800
I1206 10:40:47.728857  2840 layer_factory.hpp:77] Creating layer ip2
I1206 10:40:47.728914  2840 net.cpp:100] Creating Layer ip2
I1206 10:40:47.728946  2840 net.cpp:434] ip2 <- ip1
I1206 10:40:47.728986  2840 net.cpp:408] ip2 -> ip2
I1206 10:40:47.729912  2840 net.cpp:150] Setting up ip2
I1206 10:40:47.730042  2840 net.cpp:157] Top shape: 100 10 (1000)
I1206 10:40:47.730197  2840 net.cpp:165] Memory required for data: 8078800
I1206 10:40:47.730247  2840 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I1206 10:40:47.730376  2840 net.cpp:100] Creating Layer ip2_ip2_0_split
I1206 10:40:47.730412  2840 net.cpp:434] ip2_ip2_0_split <- ip2
I1206 10:40:47.730455  2840 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1206 10:40:47.730504  2840 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1206 10:40:47.730882  2840 net.cpp:150] Setting up ip2_ip2_0_split
I1206 10:40:47.730934  2840 net.cpp:157] Top shape: 100 10 (1000)
I1206 10:40:47.730973  2840 net.cpp:157] Top shape: 100 10 (1000)
I1206 10:40:47.731006  2840 net.cpp:165] Memory required for data: 8086800
I1206 10:40:47.731040  2840 layer_factory.hpp:77] Creating layer accuracy
I1206 10:40:47.731099  2840 net.cpp:100] Creating Layer accuracy
I1206 10:40:47.731135  2840 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I1206 10:40:47.731173  2840 net.cpp:434] accuracy <- label_mnist_1_split_0
I1206 10:40:47.731210  2840 net.cpp:408] accuracy -> accuracy
I1206 10:40:47.731256  2840 net.cpp:150] Setting up accuracy
I1206 10:40:47.731288  2840 net.cpp:157] Top shape: (1)
I1206 10:40:47.731323  2840 net.cpp:165] Memory required for data: 8086804
I1206 10:40:47.731354  2840 layer_factory.hpp:77] Creating layer loss
I1206 10:40:47.731395  2840 net.cpp:100] Creating Layer loss
I1206 10:40:47.731485  2840 net.cpp:434] loss <- ip2_ip2_0_split_1
I1206 10:40:47.731523  2840 net.cpp:434] loss <- label_mnist_1_split_1
I1206 10:40:47.731559  2840 net.cpp:408] loss -> loss
I1206 10:40:47.731601  2840 layer_factory.hpp:77] Creating layer loss
I1206 10:40:47.733909  2840 net.cpp:150] Setting up loss
I1206 10:40:47.733963  2840 net.cpp:157] Top shape: (1)
I1206 10:40:47.733999  2840 net.cpp:160]     with loss weight 1
I1206 10:40:47.734040  2840 net.cpp:165] Memory required for data: 8086808
I1206 10:40:47.734076  2840 net.cpp:226] loss needs backward computation.
I1206 10:40:47.734108  2840 net.cpp:228] accuracy does not need backward computation.
I1206 10:40:47.734143  2840 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1206 10:40:47.734175  2840 net.cpp:226] ip2 needs backward computation.
I1206 10:40:47.734208  2840 net.cpp:226] relu1 needs backward computation.
I1206 10:40:47.734241  2840 net.cpp:226] ip1 needs backward computation.
I1206 10:40:47.734275  2840 net.cpp:226] pool2 needs backward computation.
I1206 10:40:47.734308  2840 net.cpp:226] conv2 needs backward computation.
I1206 10:40:47.734341  2840 net.cpp:226] pool1 needs backward computation.
I1206 10:40:47.734375  2840 net.cpp:226] conv1 needs backward computation.
I1206 10:40:47.734406  2840 net.cpp:228] label_mnist_1_split does not need backward computation.
I1206 10:40:47.734436  2840 net.cpp:228] mnist does not need backward computation.
I1206 10:40:47.734464  2840 net.cpp:270] This network produces output accuracy
I1206 10:40:47.734498  2840 net.cpp:270] This network produces output loss
I1206 10:40:47.734557  2840 net.cpp:283] Network initialization done.
I1206 10:40:47.734714  2840 solver.cpp:72] Solver scaffolding done.
I1206 10:40:47.735728  2840 caffe.cpp:251] Starting Optimization
I1206 10:40:47.735770  2840 solver.cpp:291] Solving LeNet
I1206 10:40:47.735802  2840 solver.cpp:292] Learning Rate Policy: inv
I1206 10:40:47.738862  2840 solver.cpp:349] Iteration 0, Testing net (#0)
I1206 10:40:48.409334  2840 solver.cpp:416]     Test net output #0: accuracy = 0.0937
I1206 10:40:48.409458  2840 solver.cpp:416]     Test net output #1: loss = 79.1457 (* 1 = 79.1457 loss)
I1206 10:40:48.428741  2840 solver.cpp:240] Iteration 0, loss = 76.4195
I1206 10:40:48.428845  2840 solver.cpp:256]     Train net output #0: loss = 76.4195 (* 1 = 76.4195 loss)
I1206 10:40:48.428936  2840 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1206 10:40:50.001396  2840 solver.cpp:240] Iteration 100, loss = 87.3365
I1206 10:40:50.001543  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:40:50.001624  2840 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I1206 10:40:51.632446  2840 solver.cpp:240] Iteration 200, loss = 87.3365
I1206 10:40:51.632658  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:40:51.632756  2840 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I1206 10:40:53.228129  2840 solver.cpp:240] Iteration 300, loss = 87.3365
I1206 10:40:53.228265  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:40:53.228341  2840 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I1206 10:40:54.769516  2840 solver.cpp:240] Iteration 400, loss = 87.3365
I1206 10:40:54.769698  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:40:54.769824  2840 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I1206 10:40:56.309517  2840 solver.cpp:349] Iteration 500, Testing net (#0)
I1206 10:40:56.844473  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:40:56.844559  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:40:56.849617  2840 solver.cpp:240] Iteration 500, loss = 87.3365
I1206 10:40:56.849699  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:40:56.849740  2840 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1206 10:40:58.408370  2840 solver.cpp:240] Iteration 600, loss = 87.3365
I1206 10:40:58.408535  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:40:58.408741  2840 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I1206 10:40:59.965235  2840 solver.cpp:240] Iteration 700, loss = 87.3365
I1206 10:40:59.965332  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:40:59.965387  2840 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I1206 10:41:01.616010  2840 solver.cpp:240] Iteration 800, loss = 87.3365
I1206 10:41:01.616200  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:01.616313  2840 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I1206 10:41:03.182425  2840 solver.cpp:240] Iteration 900, loss = 87.3365
I1206 10:41:03.182603  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:03.182689  2840 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I1206 10:41:04.736343  2840 solver.cpp:349] Iteration 1000, Testing net (#0)
I1206 10:41:05.296958  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:41:05.297056  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:05.302656  2840 solver.cpp:240] Iteration 1000, loss = 87.3365
I1206 10:41:05.302749  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:05.302811  2840 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I1206 10:41:06.920567  2840 solver.cpp:240] Iteration 1100, loss = 87.3365
I1206 10:41:06.920702  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:06.920790  2840 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I1206 10:41:08.474848  2840 solver.cpp:240] Iteration 1200, loss = 87.3365
I1206 10:41:08.474987  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:08.475075  2840 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I1206 10:41:10.065208  2840 solver.cpp:240] Iteration 1300, loss = 87.3365
I1206 10:41:10.065371  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:10.065471  2840 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I1206 10:41:11.608817  2840 solver.cpp:240] Iteration 1400, loss = 87.3365
I1206 10:41:11.608969  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:11.609048  2840 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I1206 10:41:13.163839  2840 solver.cpp:349] Iteration 1500, Testing net (#0)
I1206 10:41:13.851219  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:41:13.851318  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:13.858325  2840 solver.cpp:240] Iteration 1500, loss = 87.3365
I1206 10:41:13.858420  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:13.858610  2840 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I1206 10:41:15.535882  2840 solver.cpp:240] Iteration 1600, loss = 87.3365
I1206 10:41:15.536478  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:15.536566  2840 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I1206 10:41:17.199131  2840 solver.cpp:240] Iteration 1700, loss = 87.3365
I1206 10:41:17.199260  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:17.199337  2840 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I1206 10:41:18.838619  2840 solver.cpp:240] Iteration 1800, loss = 87.3365
I1206 10:41:18.838776  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:18.838865  2840 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I1206 10:41:20.486665  2840 solver.cpp:240] Iteration 1900, loss = 87.3365
I1206 10:41:20.487010  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:20.487218  2840 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I1206 10:41:22.039831  2840 solver.cpp:349] Iteration 2000, Testing net (#0)
I1206 10:41:22.610792  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:41:22.610893  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:22.615718  2840 solver.cpp:240] Iteration 2000, loss = 87.3365
I1206 10:41:22.615826  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:22.615890  2840 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I1206 10:41:24.202363  2840 solver.cpp:240] Iteration 2100, loss = 87.3365
I1206 10:41:24.202550  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:24.202646  2840 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I1206 10:41:25.802958  2840 solver.cpp:240] Iteration 2200, loss = 87.3365
I1206 10:41:25.803107  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:25.803189  2840 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I1206 10:41:27.397228  2840 solver.cpp:240] Iteration 2300, loss = 87.3365
I1206 10:41:27.397394  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:27.397490  2840 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I1206 10:41:28.974488  2840 solver.cpp:240] Iteration 2400, loss = 87.3365
I1206 10:41:28.974642  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:28.974725  2840 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I1206 10:41:30.598956  2840 solver.cpp:349] Iteration 2500, Testing net (#0)
I1206 10:41:31.153422  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:41:31.153513  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:31.158618  2840 solver.cpp:240] Iteration 2500, loss = 87.3365
I1206 10:41:31.158697  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:31.158751  2840 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I1206 10:41:32.819514  2840 solver.cpp:240] Iteration 2600, loss = 87.3365
I1206 10:41:32.819664  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:32.819759  2840 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I1206 10:41:34.421970  2840 solver.cpp:240] Iteration 2700, loss = 87.3365
I1206 10:41:34.422119  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:34.422209  2840 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I1206 10:41:36.011929  2840 solver.cpp:240] Iteration 2800, loss = 87.3365
I1206 10:41:36.012084  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:36.012172  2840 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I1206 10:41:37.668680  2840 solver.cpp:240] Iteration 2900, loss = 87.3365
I1206 10:41:37.668823  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:37.668915  2840 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I1206 10:41:39.326828  2840 solver.cpp:349] Iteration 3000, Testing net (#0)
I1206 10:41:39.855805  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:41:39.856034  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:39.861330  2840 solver.cpp:240] Iteration 3000, loss = 87.3365
I1206 10:41:39.861593  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:39.861745  2840 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I1206 10:41:41.450465  2840 solver.cpp:240] Iteration 3100, loss = 87.3365
I1206 10:41:41.450623  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:41.450712  2840 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I1206 10:41:43.108896  2840 solver.cpp:240] Iteration 3200, loss = 87.3365
I1206 10:41:43.109010  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:43.109081  2840 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I1206 10:41:44.714592  2840 solver.cpp:240] Iteration 3300, loss = 87.3365
I1206 10:41:44.714736  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:44.714824  2840 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I1206 10:41:46.325076  2840 solver.cpp:240] Iteration 3400, loss = 87.3365
I1206 10:41:46.325908  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:46.326218  2840 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I1206 10:41:47.860218  2840 solver.cpp:349] Iteration 3500, Testing net (#0)
I1206 10:41:48.428385  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:41:48.428490  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:48.434942  2840 solver.cpp:240] Iteration 3500, loss = 87.3365
I1206 10:41:48.435021  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:48.435070  2840 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I1206 10:41:50.063694  2840 solver.cpp:240] Iteration 3600, loss = 87.3365
I1206 10:41:50.063841  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:50.063938  2840 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I1206 10:41:51.727054  2840 solver.cpp:240] Iteration 3700, loss = 87.3365
I1206 10:41:51.727164  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:51.727238  2840 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I1206 10:41:53.293778  2840 solver.cpp:240] Iteration 3800, loss = 87.3365
I1206 10:41:53.293920  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:53.294003  2840 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I1206 10:41:54.833456  2840 solver.cpp:240] Iteration 3900, loss = 87.3365
I1206 10:41:54.833621  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:54.833719  2840 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I1206 10:41:56.394222  2840 solver.cpp:349] Iteration 4000, Testing net (#0)
I1206 10:41:56.933261  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:41:56.933490  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:56.938309  2840 solver.cpp:240] Iteration 4000, loss = 87.3365
I1206 10:41:56.938513  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:56.938640  2840 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I1206 10:41:58.513042  2840 solver.cpp:240] Iteration 4100, loss = 87.3365
I1206 10:41:58.513208  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:41:58.513314  2840 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I1206 10:42:00.101076  2840 solver.cpp:240] Iteration 4200, loss = 87.3365
I1206 10:42:00.101394  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:00.101586  2840 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I1206 10:42:01.683915  2840 solver.cpp:240] Iteration 4300, loss = 87.3365
I1206 10:42:01.684065  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:01.684159  2840 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I1206 10:42:03.240914  2840 solver.cpp:240] Iteration 4400, loss = 87.3365
I1206 10:42:03.241046  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:03.241116  2840 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I1206 10:42:04.841331  2840 solver.cpp:349] Iteration 4500, Testing net (#0)
I1206 10:42:05.441733  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:42:05.441844  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:05.448087  2840 solver.cpp:240] Iteration 4500, loss = 87.3365
I1206 10:42:05.448215  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:05.448318  2840 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I1206 10:42:07.114114  2840 solver.cpp:240] Iteration 4600, loss = 87.3365
I1206 10:42:07.114233  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:07.114320  2840 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I1206 10:42:08.773159  2840 solver.cpp:240] Iteration 4700, loss = 87.3365
I1206 10:42:08.773283  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:08.773365  2840 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I1206 10:42:10.346345  2840 solver.cpp:240] Iteration 4800, loss = 87.3365
I1206 10:42:10.346510  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:10.346608  2840 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I1206 10:42:11.889591  2840 solver.cpp:240] Iteration 4900, loss = 87.3365
I1206 10:42:11.889739  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:11.889814  2840 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I1206 10:42:13.416659  2840 solver.cpp:466] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I1206 10:42:13.476054  2840 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I1206 10:42:13.487730  2840 solver.cpp:349] Iteration 5000, Testing net (#0)
I1206 10:42:14.082538  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:42:14.082631  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:14.087229  2840 solver.cpp:240] Iteration 5000, loss = 87.3365
I1206 10:42:14.087309  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:14.087352  2840 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788
I1206 10:42:15.644942  2840 solver.cpp:240] Iteration 5100, loss = 87.3365
I1206 10:42:15.645081  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:15.645156  2840 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412
I1206 10:42:17.228513  2840 solver.cpp:240] Iteration 5200, loss = 87.3365
I1206 10:42:17.228837  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:17.228958  2840 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495
I1206 10:42:18.771992  2840 solver.cpp:240] Iteration 5300, loss = 87.3365
I1206 10:42:18.772140  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:18.772218  2840 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911
I1206 10:42:20.341162  2840 solver.cpp:240] Iteration 5400, loss = 87.3365
I1206 10:42:20.341336  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:20.341435  2840 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368
I1206 10:42:21.864217  2840 solver.cpp:349] Iteration 5500, Testing net (#0)
I1206 10:42:22.419666  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:42:22.419895  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:22.425431  2840 solver.cpp:240] Iteration 5500, loss = 87.3365
I1206 10:42:22.425624  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:22.425789  2840 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865
I1206 10:42:23.977840  2840 solver.cpp:240] Iteration 5600, loss = 87.3365
I1206 10:42:23.978005  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:23.978127  2840 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402
I1206 10:42:25.563088  2840 solver.cpp:240] Iteration 5700, loss = 87.3365
I1206 10:42:25.563228  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:25.563315  2840 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977
I1206 10:42:27.134491  2840 solver.cpp:240] Iteration 5800, loss = 87.3365
I1206 10:42:27.134634  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:27.134711  2840 sgd_solver.cpp:106] Iteration 5800, lr = 0.00709589
I1206 10:42:28.726475  2840 solver.cpp:240] Iteration 5900, loss = 87.3365
I1206 10:42:28.726634  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:28.726740  2840 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624
I1206 10:42:30.308470  2840 solver.cpp:349] Iteration 6000, Testing net (#0)
I1206 10:42:30.846912  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:42:30.847002  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:30.853108  2840 solver.cpp:240] Iteration 6000, loss = 87.3365
I1206 10:42:30.853191  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:30.853260  2840 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927
I1206 10:42:32.465431  2840 solver.cpp:240] Iteration 6100, loss = 87.3365
I1206 10:42:32.465575  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:32.465673  2840 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965
I1206 10:42:34.035763  2840 solver.cpp:240] Iteration 6200, loss = 87.3365
I1206 10:42:34.035904  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:34.035985  2840 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408
I1206 10:42:35.617617  2840 solver.cpp:240] Iteration 6300, loss = 87.3365
I1206 10:42:35.617780  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:35.617888  2840 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201
I1206 10:42:37.187453  2840 solver.cpp:240] Iteration 6400, loss = 87.3365
I1206 10:42:37.187605  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:37.187705  2840 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029
I1206 10:42:38.763236  2840 solver.cpp:349] Iteration 6500, Testing net (#0)
I1206 10:42:39.278987  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:42:39.279076  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:39.283471  2840 solver.cpp:240] Iteration 6500, loss = 87.3365
I1206 10:42:39.283552  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:39.283602  2840 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689
I1206 10:42:40.826243  2840 solver.cpp:240] Iteration 6600, loss = 87.3365
I1206 10:42:40.826400  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:40.826488  2840 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784
I1206 10:42:42.409056  2840 solver.cpp:240] Iteration 6700, loss = 87.3365
I1206 10:42:42.409207  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:42.409296  2840 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711
I1206 10:42:43.988945  2840 solver.cpp:240] Iteration 6800, loss = 87.3365
I1206 10:42:43.989087  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:43.989176  2840 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767
I1206 10:42:45.558002  2840 solver.cpp:240] Iteration 6900, loss = 87.3365
I1206 10:42:45.558143  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:45.558238  2840 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466
I1206 10:42:47.107823  2840 solver.cpp:349] Iteration 7000, Testing net (#0)
I1206 10:42:47.623240  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:42:47.623625  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:47.629184  2840 solver.cpp:240] Iteration 7000, loss = 87.3365
I1206 10:42:47.629277  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:47.629333  2840 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681
I1206 10:42:49.218545  2840 solver.cpp:240] Iteration 7100, loss = 87.3365
I1206 10:42:49.218694  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:49.218782  2840 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733
I1206 10:42:50.808383  2840 solver.cpp:240] Iteration 7200, loss = 87.3365
I1206 10:42:50.808545  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:50.808642  2840 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815
I1206 10:42:52.381259  2840 solver.cpp:240] Iteration 7300, loss = 87.3365
I1206 10:42:52.381443  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:52.381552  2840 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927
I1206 10:42:53.946101  2840 solver.cpp:240] Iteration 7400, loss = 87.3365
I1206 10:42:53.946270  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:53.946378  2840 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067
I1206 10:42:55.502899  2840 solver.cpp:349] Iteration 7500, Testing net (#0)
I1206 10:42:56.037456  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:42:56.037545  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:56.042245  2840 solver.cpp:240] Iteration 7500, loss = 87.3365
I1206 10:42:56.042325  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:56.042373  2840 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236
I1206 10:42:57.575616  2840 solver.cpp:240] Iteration 7600, loss = 87.3365
I1206 10:42:57.576055  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:57.576359  2840 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433
I1206 10:42:59.105345  2840 solver.cpp:240] Iteration 7700, loss = 87.3365
I1206 10:42:59.105515  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:42:59.105638  2840 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658
I1206 10:43:00.646344  2840 solver.cpp:240] Iteration 7800, loss = 87.3365
I1206 10:43:00.646486  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:00.646590  2840 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911
I1206 10:43:02.249575  2840 solver.cpp:240] Iteration 7900, loss = 87.3365
I1206 10:43:02.249754  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:02.249856  2840 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619
I1206 10:43:03.851938  2840 solver.cpp:349] Iteration 8000, Testing net (#0)
I1206 10:43:04.365125  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:43:04.365214  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:04.369731  2840 solver.cpp:240] Iteration 8000, loss = 87.3365
I1206 10:43:04.369814  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:04.369858  2840 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496
I1206 10:43:06.014803  2840 solver.cpp:240] Iteration 8100, loss = 87.3365
I1206 10:43:06.014927  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:06.015007  2840 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827
I1206 10:43:07.665216  2840 solver.cpp:240] Iteration 8200, loss = 87.3365
I1206 10:43:07.665356  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:07.665446  2840 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185
I1206 10:43:09.285826  2840 solver.cpp:240] Iteration 8300, loss = 87.3365
I1206 10:43:09.285967  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:09.286155  2840 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567
I1206 10:43:10.929818  2840 solver.cpp:240] Iteration 8400, loss = 87.3365
I1206 10:43:10.929954  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:10.930039  2840 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975
I1206 10:43:12.563606  2840 solver.cpp:349] Iteration 8500, Testing net (#0)
I1206 10:43:13.117769  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:43:13.117882  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:13.123860  2840 solver.cpp:240] Iteration 8500, loss = 87.3365
I1206 10:43:13.123965  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:13.124032  2840 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407
I1206 10:43:14.745826  2840 solver.cpp:240] Iteration 8600, loss = 87.3365
I1206 10:43:14.746001  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:14.746119  2840 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864
I1206 10:43:16.274569  2840 solver.cpp:240] Iteration 8700, loss = 87.3365
I1206 10:43:16.274732  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:16.274837  2840 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344
I1206 10:43:17.815026  2840 solver.cpp:240] Iteration 8800, loss = 87.3365
I1206 10:43:17.815822  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:17.815932  2840 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847
I1206 10:43:19.344012  2840 solver.cpp:240] Iteration 8900, loss = 87.3365
I1206 10:43:19.344185  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:19.344295  2840 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374
I1206 10:43:20.890524  2840 solver.cpp:349] Iteration 9000, Testing net (#0)
I1206 10:43:21.419555  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:43:21.419648  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:21.424191  2840 solver.cpp:240] Iteration 9000, loss = 87.3365
I1206 10:43:21.424270  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:21.424321  2840 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924
I1206 10:43:23.003255  2840 solver.cpp:240] Iteration 9100, loss = 87.3365
I1206 10:43:23.003412  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:23.003509  2840 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496
I1206 10:43:24.575088  2840 solver.cpp:240] Iteration 9200, loss = 87.3365
I1206 10:43:24.575244  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:24.575341  2840 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309
I1206 10:43:26.144661  2840 solver.cpp:240] Iteration 9300, loss = 87.3365
I1206 10:43:26.144820  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:26.144906  2840 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706
I1206 10:43:27.715322  2840 solver.cpp:240] Iteration 9400, loss = 87.3365
I1206 10:43:27.715498  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:27.715613  2840 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343
I1206 10:43:29.278511  2840 solver.cpp:349] Iteration 9500, Testing net (#0)
I1206 10:43:29.810119  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:43:29.810220  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:29.815997  2840 solver.cpp:240] Iteration 9500, loss = 87.3365
I1206 10:43:29.816089  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:29.816140  2840 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002
I1206 10:43:31.420814  2840 solver.cpp:240] Iteration 9600, loss = 87.3365
I1206 10:43:31.420985  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:31.421090  2840 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682
I1206 10:43:33.004932  2840 solver.cpp:240] Iteration 9700, loss = 87.3365
I1206 10:43:33.005342  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:33.005611  2840 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382
I1206 10:43:34.560315  2840 solver.cpp:240] Iteration 9800, loss = 87.3365
I1206 10:43:34.560474  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:34.560559  2840 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102
I1206 10:43:36.128139  2840 solver.cpp:240] Iteration 9900, loss = 87.3365
I1206 10:43:36.128304  2840 solver.cpp:256]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:36.128415  2840 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843
I1206 10:43:37.658252  2840 solver.cpp:466] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I1206 10:43:37.710937  2840 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I1206 10:43:37.725255  2840 solver.cpp:329] Iteration 10000, loss = 87.3365
I1206 10:43:37.725345  2840 solver.cpp:349] Iteration 10000, Testing net (#0)
I1206 10:43:38.329237  2840 solver.cpp:416]     Test net output #0: accuracy = 0.1009
I1206 10:43:38.329360  2840 solver.cpp:416]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1206 10:43:38.329529  2840 solver.cpp:334] Optimization Done.
I1206 10:43:38.329584  2840 caffe.cpp:254] Optimization Done.
